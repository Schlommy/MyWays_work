{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Mudit Soni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HW0CmWMY14t8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read data and correct errors\n",
    "data= pd.read_csv('TedoneItemAssignmentTable incomplete.csv')\n",
    "test_data= pd.read_csv('Enneagram questions for test 23.11.19.csv')\n",
    "bad_idx= data.index[data['Unnamed: 4']=='Achievement-striving'].tolist()\n",
    "for idx in bad_idx:\n",
    "    data.loc[idx, 'Unnamed: 4']= 'Achievement-Striving'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "id": "mVfcgCjP3dL6",
    "outputId": "dcf48ea5-3013-431f-a3e4-c21a557b7f42"
   },
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "from sentence_embedder import *\n",
    "\n",
    "embedder= SentenceEmbedder()\n",
    "size= data.shape[0]\n",
    "class_scores= {str(x): [] for x in range(1, 10)}\n",
    "X_txt= []\n",
    "X= []\n",
    "sentiment= []\n",
    "\n",
    "for category in data['Unnamed: 4'].unique():\n",
    "    current_scores= {str(x): 0 for x in range(1, 10)}\n",
    "    rows= data.index[data['Unnamed: 4']==category].tolist()\n",
    "    first= True\n",
    "    for row in rows:\n",
    "        X_txt.append(data.loc[row, 'Unnamed: 3'])\n",
    "        X.append(embedder.get(data.loc[row, 'Unnamed: 3']))\n",
    "        sentiment.append(data.loc[row, 'Unnamed: 2'])\n",
    "        if first==True:\n",
    "            for j in [3, 2, 1, -1]:\n",
    "                try:\n",
    "                    for i in data.loc[row, str(j)].split(','):\n",
    "                        current_scores[i]=j #if data.loc[row, 'Unnamed: 2']==1 else -j\n",
    "                except: \n",
    "                    continue\n",
    "            first= False\n",
    "        for key in class_scores.keys():\n",
    "            class_scores[key].append(current_scores[key] if data.loc[row, 'Unnamed: 2']==1 else -current_scores[key])\n",
    "\n",
    "class_scores_mod= {}\n",
    "for key in class_scores.keys():\n",
    "    class_scores[key]= np.array(class_scores[key])\n",
    "    class_scores_mod[key]= np.zeros((size, 7))\n",
    "    class_scores_mod[key][np.arange(size), class_scores[key]+3]= 1\n",
    "\n",
    "X= np.array(X)\n",
    "X_txt= np.array(X_txt)\n",
    "sentiment= np.array(sentiment)\n",
    "sents= np.vstack((sentiment==1, sentiment==-1)).astype(float).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is the model for classifying negative and positive sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dKXSSFH8kNy4"
   },
   "outputs": [],
   "source": [
    "def classifier_model():\n",
    "    inp = Input(shape=(512,))\n",
    "    x = Dense(64, activation='relu')(inp)\n",
    "    outp = Dense(2, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(optimizer='adam', loss= 'categorical_crossentropy', \n",
    "                    metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VGUG3CKsonTR"
   },
   "outputs": [],
   "source": [
    "# Prepare the data.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_sent_tr, x_sent_te, sent_tr, sent_te= train_test_split(X, sents, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "B-9HM3MVpSnW",
    "outputId": "856c3f68-3b19-4099-c312-3297c5ad7094"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1020 samples, validate on 114 samples\n",
      "Epoch 1/200\n",
      "1020/1020 [==============================] - 0s 145us/sample - loss: 0.6696 - acc: 0.6049 - val_loss: 0.6767 - val_acc: 0.5526\n",
      "Epoch 2/200\n",
      "1020/1020 [==============================] - 0s 68us/sample - loss: 0.6373 - acc: 0.6147 - val_loss: 0.6601 - val_acc: 0.5526\n",
      "Epoch 3/200\n",
      "1020/1020 [==============================] - 0s 64us/sample - loss: 0.6044 - acc: 0.6627 - val_loss: 0.6394 - val_acc: 0.6140\n",
      "Epoch 4/200\n",
      "1020/1020 [==============================] - 0s 54us/sample - loss: 0.5750 - acc: 0.6912 - val_loss: 0.6329 - val_acc: 0.6316\n",
      "Epoch 5/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.5477 - acc: 0.7265 - val_loss: 0.6341 - val_acc: 0.6140\n",
      "Epoch 6/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.5260 - acc: 0.7471 - val_loss: 0.6277 - val_acc: 0.6053\n",
      "Epoch 7/200\n",
      "1020/1020 [==============================] - 0s 62us/sample - loss: 0.5110 - acc: 0.7461 - val_loss: 0.6466 - val_acc: 0.6316\n",
      "Epoch 8/200\n",
      "1020/1020 [==============================] - 0s 63us/sample - loss: 0.4929 - acc: 0.7706 - val_loss: 0.6272 - val_acc: 0.6316\n",
      "Epoch 9/200\n",
      "1020/1020 [==============================] - 0s 69us/sample - loss: 0.4781 - acc: 0.7696 - val_loss: 0.6089 - val_acc: 0.6491\n",
      "Epoch 10/200\n",
      "1020/1020 [==============================] - 0s 66us/sample - loss: 0.4624 - acc: 0.7922 - val_loss: 0.6073 - val_acc: 0.6316\n",
      "Epoch 11/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.4479 - acc: 0.8010 - val_loss: 0.6415 - val_acc: 0.6404\n",
      "Epoch 12/200\n",
      "1020/1020 [==============================] - 0s 58us/sample - loss: 0.4398 - acc: 0.8049 - val_loss: 0.6174 - val_acc: 0.6579\n",
      "Epoch 13/200\n",
      "1020/1020 [==============================] - 0s 65us/sample - loss: 0.4241 - acc: 0.8069 - val_loss: 0.6018 - val_acc: 0.6316\n",
      "Epoch 14/200\n",
      "1020/1020 [==============================] - 0s 69us/sample - loss: 0.4079 - acc: 0.8294 - val_loss: 0.6405 - val_acc: 0.6491\n",
      "Epoch 15/200\n",
      "1020/1020 [==============================] - 0s 67us/sample - loss: 0.3958 - acc: 0.8382 - val_loss: 0.5941 - val_acc: 0.6579\n",
      "Epoch 16/200\n",
      "1020/1020 [==============================] - 0s 58us/sample - loss: 0.3845 - acc: 0.8402 - val_loss: 0.5792 - val_acc: 0.6842\n",
      "Epoch 17/200\n",
      "1020/1020 [==============================] - 0s 60us/sample - loss: 0.3720 - acc: 0.8480 - val_loss: 0.6134 - val_acc: 0.6754\n",
      "Epoch 18/200\n",
      "1020/1020 [==============================] - 0s 61us/sample - loss: 0.3584 - acc: 0.8637 - val_loss: 0.5781 - val_acc: 0.7105\n",
      "Epoch 19/200\n",
      "1020/1020 [==============================] - 0s 71us/sample - loss: 0.3468 - acc: 0.8716 - val_loss: 0.6235 - val_acc: 0.7018\n",
      "Epoch 20/200\n",
      "1020/1020 [==============================] - 0s 58us/sample - loss: 0.3364 - acc: 0.8706 - val_loss: 0.5733 - val_acc: 0.7193\n",
      "Epoch 21/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.3276 - acc: 0.8706 - val_loss: 0.5858 - val_acc: 0.7368\n",
      "Epoch 22/200\n",
      "1020/1020 [==============================] - 0s 60us/sample - loss: 0.3177 - acc: 0.8765 - val_loss: 0.6259 - val_acc: 0.6930\n",
      "Epoch 23/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.3034 - acc: 0.8882 - val_loss: 0.5801 - val_acc: 0.7193\n",
      "Epoch 24/200\n",
      "1020/1020 [==============================] - 0s 65us/sample - loss: 0.3024 - acc: 0.8843 - val_loss: 0.6143 - val_acc: 0.7105\n",
      "Epoch 25/200\n",
      "1020/1020 [==============================] - 0s 58us/sample - loss: 0.2944 - acc: 0.8912 - val_loss: 0.6142 - val_acc: 0.7456\n",
      "Epoch 26/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.2823 - acc: 0.9049 - val_loss: 0.5990 - val_acc: 0.7368\n",
      "Epoch 27/200\n",
      "1020/1020 [==============================] - 0s 54us/sample - loss: 0.2737 - acc: 0.9029 - val_loss: 0.6002 - val_acc: 0.7368\n",
      "Epoch 28/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.2682 - acc: 0.9069 - val_loss: 0.5939 - val_acc: 0.7456\n",
      "Epoch 29/200\n",
      "1020/1020 [==============================] - 0s 55us/sample - loss: 0.2587 - acc: 0.9088 - val_loss: 0.6423 - val_acc: 0.7193\n",
      "Epoch 30/200\n",
      "1020/1020 [==============================] - 0s 58us/sample - loss: 0.2528 - acc: 0.9059 - val_loss: 0.6485 - val_acc: 0.7368\n",
      "Epoch 31/200\n",
      "1020/1020 [==============================] - 0s 55us/sample - loss: 0.2505 - acc: 0.9088 - val_loss: 0.6023 - val_acc: 0.7456\n",
      "Epoch 32/200\n",
      "1020/1020 [==============================] - 0s 55us/sample - loss: 0.2422 - acc: 0.9216 - val_loss: 0.6466 - val_acc: 0.7456\n",
      "Epoch 33/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.2372 - acc: 0.9167 - val_loss: 0.6266 - val_acc: 0.7368\n",
      "Epoch 34/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.2299 - acc: 0.9186 - val_loss: 0.6104 - val_acc: 0.7368\n",
      "Epoch 35/200\n",
      "1020/1020 [==============================] - 0s 55us/sample - loss: 0.2267 - acc: 0.9216 - val_loss: 0.6253 - val_acc: 0.7456\n",
      "Epoch 36/200\n",
      "1020/1020 [==============================] - 0s 63us/sample - loss: 0.2207 - acc: 0.9196 - val_loss: 0.6151 - val_acc: 0.7368\n",
      "Epoch 37/200\n",
      "1020/1020 [==============================] - 0s 76us/sample - loss: 0.2169 - acc: 0.9216 - val_loss: 0.6603 - val_acc: 0.7544\n",
      "Epoch 38/200\n",
      "1020/1020 [==============================] - 0s 67us/sample - loss: 0.2132 - acc: 0.9245 - val_loss: 0.6270 - val_acc: 0.7368\n",
      "Epoch 39/200\n",
      "1020/1020 [==============================] - 0s 62us/sample - loss: 0.2087 - acc: 0.9255 - val_loss: 0.6907 - val_acc: 0.7456\n",
      "Epoch 40/200\n",
      "1020/1020 [==============================] - 0s 55us/sample - loss: 0.2084 - acc: 0.9265 - val_loss: 0.6708 - val_acc: 0.7544\n",
      "Epoch 41/200\n",
      "1020/1020 [==============================] - 0s 65us/sample - loss: 0.2007 - acc: 0.9265 - val_loss: 0.6493 - val_acc: 0.7456\n",
      "Epoch 42/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1963 - acc: 0.9324 - val_loss: 0.6812 - val_acc: 0.7456\n",
      "Epoch 43/200\n",
      "1020/1020 [==============================] - 0s 70us/sample - loss: 0.1928 - acc: 0.9324 - val_loss: 0.6848 - val_acc: 0.7544\n",
      "Epoch 44/200\n",
      "1020/1020 [==============================] - 0s 55us/sample - loss: 0.1886 - acc: 0.9373 - val_loss: 0.6551 - val_acc: 0.7281\n",
      "Epoch 45/200\n",
      "1020/1020 [==============================] - 0s 63us/sample - loss: 0.1904 - acc: 0.9392 - val_loss: 0.6512 - val_acc: 0.7368\n",
      "Epoch 46/200\n",
      "1020/1020 [==============================] - 0s 58us/sample - loss: 0.1887 - acc: 0.9304 - val_loss: 0.7042 - val_acc: 0.7368\n",
      "Epoch 47/200\n",
      "1020/1020 [==============================] - 0s 53us/sample - loss: 0.1809 - acc: 0.9343 - val_loss: 0.7003 - val_acc: 0.7456\n",
      "Epoch 48/200\n",
      "1020/1020 [==============================] - 0s 63us/sample - loss: 0.1813 - acc: 0.9314 - val_loss: 0.7541 - val_acc: 0.7368\n",
      "Epoch 49/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.1812 - acc: 0.9314 - val_loss: 0.6919 - val_acc: 0.7368\n",
      "Epoch 50/200\n",
      "1020/1020 [==============================] - 0s 63us/sample - loss: 0.1757 - acc: 0.9294 - val_loss: 0.7425 - val_acc: 0.7456\n",
      "Epoch 51/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1749 - acc: 0.9304 - val_loss: 0.7090 - val_acc: 0.7456\n",
      "Epoch 52/200\n",
      "1020/1020 [==============================] - 0s 59us/sample - loss: 0.1701 - acc: 0.9392 - val_loss: 0.6924 - val_acc: 0.7368\n",
      "Epoch 53/200\n",
      "1020/1020 [==============================] - 0s 58us/sample - loss: 0.1697 - acc: 0.9343 - val_loss: 0.7306 - val_acc: 0.7456\n",
      "Epoch 54/200\n",
      "1020/1020 [==============================] - 0s 64us/sample - loss: 0.1676 - acc: 0.9412 - val_loss: 0.7675 - val_acc: 0.7456\n",
      "Epoch 55/200\n",
      "1020/1020 [==============================] - 0s 58us/sample - loss: 0.1719 - acc: 0.9324 - val_loss: 0.6898 - val_acc: 0.7281\n",
      "Epoch 56/200\n",
      "1020/1020 [==============================] - 0s 64us/sample - loss: 0.1766 - acc: 0.9284 - val_loss: 0.7219 - val_acc: 0.7368\n",
      "Epoch 57/200\n",
      "1020/1020 [==============================] - 0s 58us/sample - loss: 0.1615 - acc: 0.9441 - val_loss: 0.7306 - val_acc: 0.7368\n",
      "Epoch 58/200\n",
      "1020/1020 [==============================] - 0s 65us/sample - loss: 0.1612 - acc: 0.9353 - val_loss: 0.7523 - val_acc: 0.7456\n",
      "Epoch 59/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.1582 - acc: 0.9363 - val_loss: 0.7694 - val_acc: 0.7456\n",
      "Epoch 60/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1561 - acc: 0.9382 - val_loss: 0.7768 - val_acc: 0.7456\n",
      "Epoch 61/200\n",
      "1020/1020 [==============================] - 0s 58us/sample - loss: 0.1602 - acc: 0.9373 - val_loss: 0.7909 - val_acc: 0.7368\n",
      "Epoch 62/200\n",
      "1020/1020 [==============================] - 0s 64us/sample - loss: 0.1552 - acc: 0.9373 - val_loss: 0.7637 - val_acc: 0.7368\n",
      "Epoch 63/200\n",
      "1020/1020 [==============================] - 0s 54us/sample - loss: 0.1535 - acc: 0.9392 - val_loss: 0.7746 - val_acc: 0.7368\n",
      "Epoch 64/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1549 - acc: 0.9402 - val_loss: 0.7642 - val_acc: 0.7368\n",
      "Epoch 65/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.1521 - acc: 0.9402 - val_loss: 0.7877 - val_acc: 0.7368\n",
      "Epoch 66/200\n",
      "1020/1020 [==============================] - 0s 64us/sample - loss: 0.1486 - acc: 0.9392 - val_loss: 0.7844 - val_acc: 0.7368\n",
      "Epoch 67/200\n",
      "1020/1020 [==============================] - 0s 64us/sample - loss: 0.1472 - acc: 0.9451 - val_loss: 0.7863 - val_acc: 0.7456\n",
      "Epoch 68/200\n",
      "1020/1020 [==============================] - 0s 58us/sample - loss: 0.1479 - acc: 0.9392 - val_loss: 0.8166 - val_acc: 0.7456\n",
      "Epoch 69/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1471 - acc: 0.9392 - val_loss: 0.7718 - val_acc: 0.7456\n",
      "Epoch 70/200\n",
      "1020/1020 [==============================] - 0s 61us/sample - loss: 0.1477 - acc: 0.9392 - val_loss: 0.8294 - val_acc: 0.7456\n",
      "Epoch 71/200\n",
      "1020/1020 [==============================] - 0s 62us/sample - loss: 0.1460 - acc: 0.9363 - val_loss: 0.8282 - val_acc: 0.7456\n",
      "Epoch 72/200\n",
      "1020/1020 [==============================] - 0s 69us/sample - loss: 0.1453 - acc: 0.9373 - val_loss: 0.8232 - val_acc: 0.7456\n",
      "Epoch 73/200\n",
      "1020/1020 [==============================] - 0s 55us/sample - loss: 0.1444 - acc: 0.9392 - val_loss: 0.7950 - val_acc: 0.7456\n",
      "Epoch 74/200\n",
      "1020/1020 [==============================] - 0s 55us/sample - loss: 0.1444 - acc: 0.9363 - val_loss: 0.8394 - val_acc: 0.7368\n",
      "Epoch 75/200\n",
      "1020/1020 [==============================] - 0s 59us/sample - loss: 0.1490 - acc: 0.9343 - val_loss: 0.7920 - val_acc: 0.7456\n",
      "Epoch 76/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1414 - acc: 0.9373 - val_loss: 0.7844 - val_acc: 0.7456\n",
      "Epoch 77/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.1397 - acc: 0.9402 - val_loss: 0.8391 - val_acc: 0.7368\n",
      "Epoch 78/200\n",
      "1020/1020 [==============================] - 0s 54us/sample - loss: 0.1413 - acc: 0.9402 - val_loss: 0.8001 - val_acc: 0.7456\n",
      "Epoch 79/200\n",
      "1020/1020 [==============================] - 0s 68us/sample - loss: 0.1391 - acc: 0.9431 - val_loss: 0.8213 - val_acc: 0.7456\n",
      "Epoch 80/200\n",
      "1020/1020 [==============================] - 0s 67us/sample - loss: 0.1429 - acc: 0.9324 - val_loss: 0.8432 - val_acc: 0.7368\n",
      "Epoch 81/200\n",
      "1020/1020 [==============================] - 0s 54us/sample - loss: 0.1357 - acc: 0.9392 - val_loss: 0.8042 - val_acc: 0.7456\n",
      "Epoch 82/200\n",
      "1020/1020 [==============================] - 0s 63us/sample - loss: 0.1382 - acc: 0.9431 - val_loss: 0.8057 - val_acc: 0.7368\n",
      "Epoch 83/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.1382 - acc: 0.9402 - val_loss: 0.8285 - val_acc: 0.7368\n",
      "Epoch 84/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1343 - acc: 0.9422 - val_loss: 0.8208 - val_acc: 0.7456\n",
      "Epoch 85/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.1341 - acc: 0.9373 - val_loss: 0.8369 - val_acc: 0.7368\n",
      "Epoch 86/200\n",
      "1020/1020 [==============================] - 0s 63us/sample - loss: 0.1342 - acc: 0.9343 - val_loss: 0.8963 - val_acc: 0.7456\n",
      "Epoch 87/200\n",
      "1020/1020 [==============================] - 0s 66us/sample - loss: 0.1374 - acc: 0.9392 - val_loss: 0.8708 - val_acc: 0.7456\n",
      "Epoch 88/200\n",
      "1020/1020 [==============================] - 0s 73us/sample - loss: 0.1330 - acc: 0.9402 - val_loss: 0.8379 - val_acc: 0.7368\n",
      "Epoch 89/200\n",
      "1020/1020 [==============================] - 0s 64us/sample - loss: 0.1343 - acc: 0.9412 - val_loss: 0.8818 - val_acc: 0.7368\n",
      "Epoch 90/200\n",
      "1020/1020 [==============================] - 0s 65us/sample - loss: 0.1361 - acc: 0.9373 - val_loss: 0.8884 - val_acc: 0.7456\n",
      "Epoch 91/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1393 - acc: 0.9333 - val_loss: 0.9055 - val_acc: 0.7368\n",
      "Epoch 92/200\n",
      "1020/1020 [==============================] - 0s 58us/sample - loss: 0.1327 - acc: 0.9373 - val_loss: 0.8746 - val_acc: 0.7456\n",
      "Epoch 93/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1313 - acc: 0.9402 - val_loss: 0.8808 - val_acc: 0.7456\n",
      "Epoch 94/200\n",
      "1020/1020 [==============================] - 0s 55us/sample - loss: 0.1330 - acc: 0.9353 - val_loss: 0.8686 - val_acc: 0.7368\n",
      "Epoch 95/200\n",
      "1020/1020 [==============================] - 0s 55us/sample - loss: 0.1329 - acc: 0.9343 - val_loss: 0.8619 - val_acc: 0.7368\n",
      "Epoch 96/200\n",
      "1020/1020 [==============================] - 0s 58us/sample - loss: 0.1312 - acc: 0.9431 - val_loss: 0.8408 - val_acc: 0.7368\n",
      "Epoch 97/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1324 - acc: 0.9343 - val_loss: 0.8363 - val_acc: 0.7368\n",
      "Epoch 98/200\n",
      "1020/1020 [==============================] - 0s 55us/sample - loss: 0.1333 - acc: 0.9333 - val_loss: 0.9042 - val_acc: 0.7281\n",
      "Epoch 99/200\n",
      "1020/1020 [==============================] - 0s 58us/sample - loss: 0.1314 - acc: 0.9373 - val_loss: 0.9122 - val_acc: 0.7456\n",
      "Epoch 100/200\n",
      "1020/1020 [==============================] - 0s 58us/sample - loss: 0.1285 - acc: 0.9382 - val_loss: 0.8865 - val_acc: 0.7368\n",
      "Epoch 101/200\n",
      "1020/1020 [==============================] - 0s 59us/sample - loss: 0.1272 - acc: 0.9382 - val_loss: 0.8731 - val_acc: 0.7368\n",
      "Epoch 102/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1270 - acc: 0.9373 - val_loss: 0.8887 - val_acc: 0.7368\n",
      "Epoch 103/200\n",
      "1020/1020 [==============================] - 0s 55us/sample - loss: 0.1333 - acc: 0.9353 - val_loss: 0.8961 - val_acc: 0.7456\n",
      "Epoch 104/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1297 - acc: 0.9431 - val_loss: 0.8852 - val_acc: 0.7368\n",
      "Epoch 105/200\n",
      "1020/1020 [==============================] - 0s 55us/sample - loss: 0.1261 - acc: 0.9422 - val_loss: 0.9090 - val_acc: 0.7456\n",
      "Epoch 106/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1279 - acc: 0.9402 - val_loss: 0.9587 - val_acc: 0.7368\n",
      "Epoch 107/200\n",
      "1020/1020 [==============================] - 0s 64us/sample - loss: 0.1271 - acc: 0.9363 - val_loss: 0.9120 - val_acc: 0.7456\n",
      "Epoch 108/200\n",
      "1020/1020 [==============================] - 0s 55us/sample - loss: 0.1291 - acc: 0.9402 - val_loss: 0.9203 - val_acc: 0.7368\n",
      "Epoch 109/200\n",
      "1020/1020 [==============================] - 0s 55us/sample - loss: 0.1285 - acc: 0.9353 - val_loss: 0.8971 - val_acc: 0.7368\n",
      "Epoch 110/200\n",
      "1020/1020 [==============================] - 0s 54us/sample - loss: 0.1272 - acc: 0.9471 - val_loss: 0.8648 - val_acc: 0.7368\n",
      "Epoch 111/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1306 - acc: 0.9343 - val_loss: 0.9340 - val_acc: 0.7368\n",
      "Epoch 112/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.1282 - acc: 0.9373 - val_loss: 0.9683 - val_acc: 0.7281\n",
      "Epoch 113/200\n",
      "1020/1020 [==============================] - 0s 62us/sample - loss: 0.1253 - acc: 0.9363 - val_loss: 0.9189 - val_acc: 0.7456\n",
      "Epoch 114/200\n",
      "1020/1020 [==============================] - 0s 62us/sample - loss: 0.1262 - acc: 0.9382 - val_loss: 0.8822 - val_acc: 0.7368\n",
      "Epoch 115/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.1281 - acc: 0.9363 - val_loss: 0.9272 - val_acc: 0.7456\n",
      "Epoch 116/200\n",
      "1020/1020 [==============================] - 0s 62us/sample - loss: 0.1284 - acc: 0.9382 - val_loss: 0.9572 - val_acc: 0.7368\n",
      "Epoch 117/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.1290 - acc: 0.9343 - val_loss: 0.8885 - val_acc: 0.7368\n",
      "Epoch 118/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.1234 - acc: 0.9431 - val_loss: 0.9294 - val_acc: 0.7368\n",
      "Epoch 119/200\n",
      "1020/1020 [==============================] - 0s 68us/sample - loss: 0.1350 - acc: 0.9412 - val_loss: 0.8831 - val_acc: 0.7281\n",
      "Epoch 120/200\n",
      "1020/1020 [==============================] - 0s 67us/sample - loss: 0.1307 - acc: 0.9343 - val_loss: 0.9713 - val_acc: 0.7456\n",
      "Epoch 121/200\n",
      "1020/1020 [==============================] - 0s 54us/sample - loss: 0.1256 - acc: 0.9343 - val_loss: 0.9964 - val_acc: 0.7281\n",
      "Epoch 122/200\n",
      "1020/1020 [==============================] - 0s 53us/sample - loss: 0.1276 - acc: 0.9422 - val_loss: 0.9723 - val_acc: 0.7456\n",
      "Epoch 123/200\n",
      "1020/1020 [==============================] - 0s 58us/sample - loss: 0.1238 - acc: 0.9363 - val_loss: 0.9877 - val_acc: 0.7368\n",
      "Epoch 124/200\n",
      "1020/1020 [==============================] - 0s 68us/sample - loss: 0.1280 - acc: 0.9373 - val_loss: 1.0129 - val_acc: 0.7281\n",
      "Epoch 125/200\n",
      "1020/1020 [==============================] - 0s 64us/sample - loss: 0.1254 - acc: 0.9392 - val_loss: 0.9752 - val_acc: 0.7281\n",
      "Epoch 126/200\n",
      "1020/1020 [==============================] - 0s 66us/sample - loss: 0.1235 - acc: 0.9392 - val_loss: 0.9274 - val_acc: 0.7456\n",
      "Epoch 127/200\n",
      "1020/1020 [==============================] - 0s 66us/sample - loss: 0.1231 - acc: 0.9382 - val_loss: 0.9181 - val_acc: 0.7368\n",
      "Epoch 128/200\n",
      "1020/1020 [==============================] - 0s 65us/sample - loss: 0.1216 - acc: 0.9402 - val_loss: 0.9564 - val_acc: 0.7456\n",
      "Epoch 129/200\n",
      "1020/1020 [==============================] - 0s 62us/sample - loss: 0.1250 - acc: 0.9373 - val_loss: 0.9592 - val_acc: 0.7456\n",
      "Epoch 130/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.1269 - acc: 0.9373 - val_loss: 0.9948 - val_acc: 0.7368\n",
      "Epoch 131/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.1225 - acc: 0.9333 - val_loss: 0.9542 - val_acc: 0.7368\n",
      "Epoch 132/200\n",
      "1020/1020 [==============================] - 0s 62us/sample - loss: 0.1195 - acc: 0.9402 - val_loss: 0.9803 - val_acc: 0.7456\n",
      "Epoch 133/200\n",
      "1020/1020 [==============================] - 0s 66us/sample - loss: 0.1260 - acc: 0.9392 - val_loss: 0.9790 - val_acc: 0.7456\n",
      "Epoch 134/200\n",
      "1020/1020 [==============================] - 0s 65us/sample - loss: 0.1230 - acc: 0.9402 - val_loss: 0.9537 - val_acc: 0.7456\n",
      "Epoch 135/200\n",
      "1020/1020 [==============================] - 0s 55us/sample - loss: 0.1245 - acc: 0.9324 - val_loss: 0.9509 - val_acc: 0.7368\n",
      "Epoch 136/200\n",
      "1020/1020 [==============================] - 0s 63us/sample - loss: 0.1218 - acc: 0.9382 - val_loss: 1.0029 - val_acc: 0.7368\n",
      "Epoch 137/200\n",
      "1020/1020 [==============================] - 0s 66us/sample - loss: 0.1291 - acc: 0.9284 - val_loss: 0.9898 - val_acc: 0.7281\n",
      "Epoch 138/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1242 - acc: 0.9373 - val_loss: 0.9677 - val_acc: 0.7456\n",
      "Epoch 139/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.1234 - acc: 0.9373 - val_loss: 0.9693 - val_acc: 0.7368\n",
      "Epoch 140/200\n",
      "1020/1020 [==============================] - 0s 65us/sample - loss: 0.1240 - acc: 0.9343 - val_loss: 0.9886 - val_acc: 0.7368\n",
      "Epoch 141/200\n",
      "1020/1020 [==============================] - 0s 69us/sample - loss: 0.1217 - acc: 0.9392 - val_loss: 0.9769 - val_acc: 0.7456\n",
      "Epoch 142/200\n",
      "1020/1020 [==============================] - 0s 68us/sample - loss: 0.1217 - acc: 0.9333 - val_loss: 1.0013 - val_acc: 0.7368\n",
      "Epoch 143/200\n",
      "1020/1020 [==============================] - 0s 64us/sample - loss: 0.1186 - acc: 0.9373 - val_loss: 0.9389 - val_acc: 0.7368\n",
      "Epoch 144/200\n",
      "1020/1020 [==============================] - 0s 63us/sample - loss: 0.1201 - acc: 0.9363 - val_loss: 0.9763 - val_acc: 0.7368\n",
      "Epoch 145/200\n",
      "1020/1020 [==============================] - 0s 58us/sample - loss: 0.1239 - acc: 0.9382 - val_loss: 1.0140 - val_acc: 0.7456\n",
      "Epoch 146/200\n",
      "1020/1020 [==============================] - 0s 62us/sample - loss: 0.1241 - acc: 0.9382 - val_loss: 0.9842 - val_acc: 0.7368\n",
      "Epoch 147/200\n",
      "1020/1020 [==============================] - 0s 61us/sample - loss: 0.1203 - acc: 0.9382 - val_loss: 1.0129 - val_acc: 0.7281\n",
      "Epoch 148/200\n",
      "1020/1020 [==============================] - 0s 65us/sample - loss: 0.1204 - acc: 0.9412 - val_loss: 0.9609 - val_acc: 0.7456\n",
      "Epoch 149/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1204 - acc: 0.9412 - val_loss: 0.9442 - val_acc: 0.7368\n",
      "Epoch 150/200\n",
      "1020/1020 [==============================] - 0s 64us/sample - loss: 0.1231 - acc: 0.9402 - val_loss: 0.9960 - val_acc: 0.7368\n",
      "Epoch 151/200\n",
      "1020/1020 [==============================] - 0s 64us/sample - loss: 0.1230 - acc: 0.9382 - val_loss: 0.9938 - val_acc: 0.7456\n",
      "Epoch 152/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.1188 - acc: 0.9392 - val_loss: 0.9677 - val_acc: 0.7368\n",
      "Epoch 153/200\n",
      "1020/1020 [==============================] - 0s 58us/sample - loss: 0.1209 - acc: 0.9412 - val_loss: 1.0250 - val_acc: 0.7368\n",
      "Epoch 154/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1251 - acc: 0.9353 - val_loss: 1.0188 - val_acc: 0.7368\n",
      "Epoch 155/200\n",
      "1020/1020 [==============================] - 0s 68us/sample - loss: 0.1190 - acc: 0.9412 - val_loss: 1.0117 - val_acc: 0.7368\n",
      "Epoch 156/200\n",
      "1020/1020 [==============================] - 0s 85us/sample - loss: 0.1201 - acc: 0.9392 - val_loss: 1.0052 - val_acc: 0.7368\n",
      "Epoch 157/200\n",
      "1020/1020 [==============================] - 0s 65us/sample - loss: 0.1194 - acc: 0.9373 - val_loss: 1.0348 - val_acc: 0.7368\n",
      "Epoch 158/200\n",
      "1020/1020 [==============================] - 0s 64us/sample - loss: 0.1237 - acc: 0.9382 - val_loss: 0.9692 - val_acc: 0.7368\n",
      "Epoch 159/200\n",
      "1020/1020 [==============================] - 0s 66us/sample - loss: 0.1250 - acc: 0.9353 - val_loss: 0.9581 - val_acc: 0.7368\n",
      "Epoch 160/200\n",
      "1020/1020 [==============================] - 0s 68us/sample - loss: 0.1225 - acc: 0.9412 - val_loss: 0.9823 - val_acc: 0.7368\n",
      "Epoch 161/200\n",
      "1020/1020 [==============================] - 0s 68us/sample - loss: 0.1176 - acc: 0.9392 - val_loss: 1.0131 - val_acc: 0.7456\n",
      "Epoch 162/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.1187 - acc: 0.9353 - val_loss: 1.0825 - val_acc: 0.7193\n",
      "Epoch 163/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.1207 - acc: 0.9373 - val_loss: 1.0208 - val_acc: 0.7368\n",
      "Epoch 164/200\n",
      "1020/1020 [==============================] - 0s 66us/sample - loss: 0.1221 - acc: 0.9392 - val_loss: 1.1221 - val_acc: 0.7193\n",
      "Epoch 165/200\n",
      "1020/1020 [==============================] - 0s 64us/sample - loss: 0.1264 - acc: 0.9294 - val_loss: 1.1050 - val_acc: 0.7193\n",
      "Epoch 166/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1224 - acc: 0.9392 - val_loss: 1.0091 - val_acc: 0.7456\n",
      "Epoch 167/200\n",
      "1020/1020 [==============================] - 0s 65us/sample - loss: 0.1221 - acc: 0.9422 - val_loss: 1.0372 - val_acc: 0.7368\n",
      "Epoch 168/200\n",
      "1020/1020 [==============================] - 0s 67us/sample - loss: 0.1207 - acc: 0.9422 - val_loss: 1.0898 - val_acc: 0.7281\n",
      "Epoch 169/200\n",
      "1020/1020 [==============================] - 0s 70us/sample - loss: 0.1194 - acc: 0.9392 - val_loss: 1.0349 - val_acc: 0.7456\n",
      "Epoch 170/200\n",
      "1020/1020 [==============================] - 0s 55us/sample - loss: 0.1245 - acc: 0.9333 - val_loss: 1.0641 - val_acc: 0.7281\n",
      "Epoch 171/200\n",
      "1020/1020 [==============================] - 0s 66us/sample - loss: 0.1198 - acc: 0.9363 - val_loss: 1.0213 - val_acc: 0.7456\n",
      "Epoch 172/200\n",
      "1020/1020 [==============================] - 0s 66us/sample - loss: 0.1176 - acc: 0.9402 - val_loss: 1.0714 - val_acc: 0.7456\n",
      "Epoch 173/200\n",
      "1020/1020 [==============================] - 0s 66us/sample - loss: 0.1164 - acc: 0.9402 - val_loss: 1.0036 - val_acc: 0.7368\n",
      "Epoch 174/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1183 - acc: 0.9412 - val_loss: 1.0572 - val_acc: 0.7281\n",
      "Epoch 175/200\n",
      "1020/1020 [==============================] - 0s 65us/sample - loss: 0.1203 - acc: 0.9402 - val_loss: 1.0154 - val_acc: 0.7456\n",
      "Epoch 176/200\n",
      "1020/1020 [==============================] - 0s 66us/sample - loss: 0.1185 - acc: 0.9412 - val_loss: 1.1258 - val_acc: 0.7105\n",
      "Epoch 177/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1236 - acc: 0.9431 - val_loss: 1.0725 - val_acc: 0.7281\n",
      "Epoch 178/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1290 - acc: 0.9294 - val_loss: 1.0600 - val_acc: 0.7456\n",
      "Epoch 179/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1257 - acc: 0.9363 - val_loss: 0.9882 - val_acc: 0.7368\n",
      "Epoch 180/200\n",
      "1020/1020 [==============================] - 0s 55us/sample - loss: 0.1189 - acc: 0.9382 - val_loss: 1.0901 - val_acc: 0.7281\n",
      "Epoch 181/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.1174 - acc: 0.9373 - val_loss: 1.0833 - val_acc: 0.7368\n",
      "Epoch 182/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1259 - acc: 0.9333 - val_loss: 1.0341 - val_acc: 0.7368\n",
      "Epoch 183/200\n",
      "1020/1020 [==============================] - 0s 53us/sample - loss: 0.1188 - acc: 0.9382 - val_loss: 1.0672 - val_acc: 0.7368\n",
      "Epoch 184/200\n",
      "1020/1020 [==============================] - 0s 54us/sample - loss: 0.1203 - acc: 0.9412 - val_loss: 1.0653 - val_acc: 0.7368\n",
      "Epoch 185/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1165 - acc: 0.9392 - val_loss: 0.9947 - val_acc: 0.7368\n",
      "Epoch 186/200\n",
      "1020/1020 [==============================] - 0s 55us/sample - loss: 0.1217 - acc: 0.9402 - val_loss: 1.0258 - val_acc: 0.7368\n",
      "Epoch 187/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.1225 - acc: 0.9382 - val_loss: 1.0629 - val_acc: 0.7368\n",
      "Epoch 188/200\n",
      "1020/1020 [==============================] - 0s 62us/sample - loss: 0.1218 - acc: 0.9373 - val_loss: 1.0585 - val_acc: 0.7368\n",
      "Epoch 189/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1176 - acc: 0.9402 - val_loss: 1.0701 - val_acc: 0.7368\n",
      "Epoch 190/200\n",
      "1020/1020 [==============================] - 0s 54us/sample - loss: 0.1192 - acc: 0.9412 - val_loss: 1.0375 - val_acc: 0.7456\n",
      "Epoch 191/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1172 - acc: 0.9363 - val_loss: 1.0707 - val_acc: 0.7368\n",
      "Epoch 192/200\n",
      "1020/1020 [==============================] - 0s 57us/sample - loss: 0.1163 - acc: 0.9382 - val_loss: 1.0606 - val_acc: 0.7368\n",
      "Epoch 193/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1193 - acc: 0.9431 - val_loss: 1.1403 - val_acc: 0.7281\n",
      "Epoch 194/200\n",
      "1020/1020 [==============================] - 0s 58us/sample - loss: 0.1174 - acc: 0.9363 - val_loss: 1.0413 - val_acc: 0.7368\n",
      "Epoch 195/200\n",
      "1020/1020 [==============================] - 0s 67us/sample - loss: 0.1180 - acc: 0.9412 - val_loss: 1.0400 - val_acc: 0.7368\n",
      "Epoch 196/200\n",
      "1020/1020 [==============================] - 0s 66us/sample - loss: 0.1176 - acc: 0.9363 - val_loss: 1.0844 - val_acc: 0.7456\n",
      "Epoch 197/200\n",
      "1020/1020 [==============================] - 0s 56us/sample - loss: 0.1175 - acc: 0.9451 - val_loss: 1.0285 - val_acc: 0.7368\n",
      "Epoch 198/200\n",
      "1020/1020 [==============================] - 0s 65us/sample - loss: 0.1181 - acc: 0.9392 - val_loss: 1.0715 - val_acc: 0.7456\n",
      "Epoch 199/200\n",
      "1020/1020 [==============================] - 0s 65us/sample - loss: 0.1145 - acc: 0.9422 - val_loss: 1.0392 - val_acc: 0.7368\n",
      "Epoch 200/200\n",
      "1020/1020 [==============================] - 0s 55us/sample - loss: 0.1190 - acc: 0.9353 - val_loss: 0.9888 - val_acc: 0.7368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3b2aca3278>"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model.\n",
    "mod= classifier_model()\n",
    "mod.fit(x_sent_tr, sent_tr, validation_data=(x_sent_te, sent_te), batch_size= 32, epochs= 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below are the models for generating class scores for sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom loss functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "EcdqpO-CbQJu"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Metric\n",
    "import tensorflow.keras.backend as K\n",
    "RANDOM_SEED = 3\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "@tf.function\n",
    "def cohen_kappa_loss(y_true, y_pred, row_label_vec, col_label_vec, weight_mat,  eps=1e-6, dtype=tf.float64):\n",
    "    labels = tf.matmul(y_true, col_label_vec)\n",
    "    weight = tf.pow(tf.tile(labels, [1, tf.shape(y_true)[1]]) - tf.tile(row_label_vec, [tf.shape(y_true)[0], 1]), 2)\n",
    "    weight /= tf.cast(tf.pow(tf.shape(y_true)[1] - 1, 2), dtype=dtype)\n",
    "    numerator = tf.reduce_sum(weight * y_pred)\n",
    "    \n",
    "    denominator = tf.reduce_sum(\n",
    "        tf.matmul(\n",
    "            tf.reduce_sum(y_true, axis=0, keepdims=True),\n",
    "            tf.matmul(weight_mat, tf.transpose(tf.reduce_sum(y_pred, axis=0, keepdims=True)))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    denominator /= tf.cast(tf.shape(y_true)[0], dtype=dtype)\n",
    "    \n",
    "    return tf.math.log(numerator / denominator + eps)\n",
    "\n",
    "class CohenKappaLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self,\n",
    "                 num_classes,\n",
    "                 name='cohen_kappa_loss',\n",
    "                 eps=1e-6,\n",
    "                 dtype=tf.float64):\n",
    "        super(CohenKappaLoss, self).__init__(name=name, reduction=tf.keras.losses.Reduction.NONE)\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.eps = eps\n",
    "        self.dtype = dtype\n",
    "        label_vec = tf.range(num_classes, dtype=dtype)\n",
    "        self.row_label_vec = tf.reshape(label_vec, [1, num_classes])\n",
    "        self.col_label_vec = tf.reshape(label_vec, [num_classes, 1])\n",
    "        self.weight_mat = tf.pow(\n",
    "            tf.tile(self.col_label_vec, [1, num_classes]) - tf.tile(self.row_label_vec, [num_classes, 1]),\n",
    "        2) / tf.cast(tf.pow(num_classes - 1, 2), dtype=dtype)\n",
    "\n",
    "\n",
    "    def call(self, y_true, y_pred, sample_weight=None):\n",
    "        return cohen_kappa_loss(\n",
    "            y_true, y_pred, self.row_label_vec, self.col_label_vec, self.weight_mat, self.eps, self.dtype\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"num_classes\": self.num_classes,\n",
    "            \"eps\": self.eps,\n",
    "            \"dtype\": self.dtype\n",
    "        }\n",
    "        base_config = super(CohenKappaLoss, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "vZwKwIb2bVE1"
   },
   "outputs": [],
   "source": [
    "class CohenKappa(Metric):\n",
    "    \"\"\"\n",
    "    This metric is copied from TensorFlow Addons\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_classes,\n",
    "                 name='cohen_kappa',\n",
    "                 weightage=None,\n",
    "                 dtype=tf.float32):\n",
    "        super(CohenKappa, self).__init__(name=name, dtype=dtype)\n",
    "\n",
    "        if weightage not in (None, 'linear', 'quadratic'):\n",
    "            raise ValueError(\"Unknown kappa weighting type.\")\n",
    "        else:\n",
    "            self.weightage = weightage\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.conf_mtx = self.add_weight(\n",
    "            'conf_mtx',\n",
    "            shape=(self.num_classes, self.num_classes),\n",
    "            initializer=tf.keras.initializers.zeros,\n",
    "            dtype=tf.int32)\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = tf.argmax(y_true, axis=1)\n",
    "        if len(y_pred.shape) == 2:\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "        \n",
    "        y_true = tf.cast(y_true, dtype=tf.int32)\n",
    "        y_pred = tf.cast(y_pred, dtype=tf.int32)\n",
    "        \n",
    "        if y_true.shape.as_list() != y_pred.shape.as_list():\n",
    "            raise ValueError(\n",
    "                \"Number of samples in y_true and y_pred are different\")\n",
    "\n",
    "        # compute the new values of the confusion matrix\n",
    "        new_conf_mtx = tf.math.confusion_matrix(\n",
    "            labels=y_true,\n",
    "            predictions=y_pred,\n",
    "            num_classes=self.num_classes,\n",
    "            weights=sample_weight)\n",
    "\n",
    "        # update the values in the original confusion matrix\n",
    "        return self.conf_mtx.assign_add(new_conf_mtx)\n",
    "    \n",
    "    def result(self):\n",
    "        nb_ratings = tf.shape(self.conf_mtx)[0]\n",
    "        weight_mtx = tf.ones([nb_ratings, nb_ratings], dtype=tf.int32)\n",
    "\n",
    "        # 2. Create a weight matrix\n",
    "        if self.weightage is None:\n",
    "            diagonal = tf.zeros([nb_ratings], dtype=tf.int32)\n",
    "            weight_mtx = tf.linalg.set_diag(weight_mtx, diagonal=diagonal)\n",
    "            weight_mtx = tf.cast(weight_mtx, dtype=tf.float32)\n",
    "\n",
    "        else:\n",
    "            weight_mtx += tf.range(nb_ratings, dtype=tf.int32)\n",
    "            weight_mtx = tf.cast(weight_mtx, dtype=tf.float32)\n",
    "\n",
    "            if self.weightage == 'linear':\n",
    "                weight_mtx = tf.abs(weight_mtx - tf.transpose(weight_mtx))\n",
    "            else:\n",
    "                weight_mtx = tf.pow((weight_mtx - tf.transpose(weight_mtx)), 2)\n",
    "            weight_mtx = tf.cast(weight_mtx, dtype=tf.float32)\n",
    "\n",
    "        # 3. Get counts\n",
    "        actual_ratings_hist = tf.reduce_sum(self.conf_mtx, axis=1)\n",
    "        pred_ratings_hist = tf.reduce_sum(self.conf_mtx, axis=0)\n",
    "\n",
    "        # 4. Get the outer product\n",
    "        out_prod = pred_ratings_hist[..., None] * \\\n",
    "                    actual_ratings_hist[None, ...]\n",
    "\n",
    "        # 5. Normalize the confusion matrix and outer product\n",
    "        conf_mtx = self.conf_mtx / tf.reduce_sum(self.conf_mtx)\n",
    "        out_prod = out_prod / tf.reduce_sum(out_prod)\n",
    "\n",
    "        conf_mtx = tf.cast(conf_mtx, dtype=tf.float32)\n",
    "        out_prod = tf.cast(out_prod, dtype=tf.float32)\n",
    "\n",
    "        # 6. Calculate Kappa score\n",
    "        numerator = tf.reduce_sum(conf_mtx * weight_mtx)\n",
    "        denominator = tf.reduce_sum(out_prod * weight_mtx)\n",
    "        kp = 1 - (numerator / denominator)\n",
    "        return kp\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"Returns the serializable config of the metric.\"\"\"\n",
    "\n",
    "        config = {\n",
    "            \"num_classes\": self.num_classes,\n",
    "            \"weightage\": self.weightage,\n",
    "        }\n",
    "        base_config = super(CohenKappa, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def reset_states(self):\n",
    "        \"\"\"Resets all of the metric state variables.\"\"\"\n",
    "\n",
    "        for v in self.variables:\n",
    "            K.set_value(\n",
    "                v, np.zeros((self.num_classes, self.num_classes), np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "sJmarRwubYEl"
   },
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ordinal regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6D0T2wNwF-1I"
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2.keras as keras\n",
    "from tensorflow.compat.v2.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.compat.v2.keras.models import Model\n",
    "\n",
    "def build_model1():\n",
    "    inp = Input(shape=(512,))\n",
    "    x = Dense(128, activation='relu')(inp)\n",
    "    #x= Dropout(0.5)(x)\n",
    "    #x= Dense(32, activation='relu')(x)\n",
    "    outp = Dense(7, activation='softmax')(x)\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(optimizer='adam', loss=CohenKappaLoss(7), \n",
    "                  metrics=[CohenKappa(num_classes=7, weightage='quadratic')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JmL8Lzi-cC_b"
   },
   "outputs": [],
   "source": [
    "# Prepare the data.\n",
    "mask = np.random.rand(size) < 0.9\n",
    "y_train, y_test= {}, {}\n",
    "\n",
    "X_train, X_test, X_txt_train, X_txt_test = X[mask], X[~mask], X_txt[mask], X_txt[~mask]\n",
    "for key in class_scores_mod.keys():\n",
    "    y_train[key], y_test[key]= class_scores_mod[key][mask], class_scores_mod[key][~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TXiOGxH3aq8J",
    "outputId": "bc1c552f-341d-4d67-8663-3e5bcfa656bc"
   },
   "outputs": [],
   "source": [
    "# Train models.\n",
    "from keras.callbacks import EarlyStopping\n",
    "earlystop = EarlyStopping(monitor = 'val_cohen_kappa', min_delta = 0, patience = 500, mode='max', restore_best_weights=True)\n",
    "\n",
    "models= {}\n",
    "for param in list(class_scores_mod.keys()):\n",
    "    model = build_model1()\n",
    "    model.fit(X_train, y_train[param], validation_data=(X_test,y_test[param]), callbacks= [earlystop], batch_size=64, epochs=500)\n",
    "    models[param]= model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: (Can be improved by babysitting training for individual models.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MzmkQHpFHA4C",
    "outputId": "f2919a6a-3493-4914-d5a5-b3110ddecc99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CM 1\n",
      "[[  0   0  62   0   0   0   0]\n",
      " [  0   0  12   3   0   1   0]\n",
      " [  0   0 130  48   0   2   0]\n",
      " [  0   0  27 479   0  11   3]\n",
      " [  0   0   2  79   0  53   7]\n",
      " [  0   0   0   1   0   8  19]\n",
      " [  0   0   0   3   0  11  70]]\n",
      "Testing CM 1\n",
      "[[ 0  0  3  2  0  0  1]\n",
      " [ 0  0  0  1  0  1  0]\n",
      " [ 0  0  7 11  0  1  0]\n",
      " [ 0  0  4 43  0  2  2]\n",
      " [ 0  0  2 10  0  7  1]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  2  0  0  1  1]]\n",
      "Training CM 2\n",
      "[[  0  21   0   4   0   0   1]\n",
      " [  0  29   0  15   0   0   1]\n",
      " [  0  18   0  20   0   0   0]\n",
      " [  0   7   0 772   0   0   5]\n",
      " [  0   0   0  12   0   0   2]\n",
      " [  0   0   0  14   0   0  57]\n",
      " [  0   1   0   4   0   0  48]]\n",
      "Testing CM 2\n",
      "[[ 0  3  0  2  0  0  0]\n",
      " [ 0  1  0  2  0  0  0]\n",
      " [ 0  0  0  3  0  0  1]\n",
      " [ 0  0  0 74  0  0  4]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  4  0  0  4]\n",
      " [ 0  0  0  2  0  0  2]]\n",
      "Training CM 3\n",
      "[[  0  34   0   6   0   0   0]\n",
      " [  0   4   0   5   0   0   0]\n",
      " [  0  56   0  42   0   0   0]\n",
      " [  0   7   0 655  19   0   4]\n",
      " [  0   0   0  41  66   0  10]\n",
      " [  0   0   0   9   5   0   7]\n",
      " [  0   0   0   5   6   0  50]]\n",
      "Testing CM 3\n",
      "[[ 0  4  0  0  0  0]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  4  0  7  0  0]\n",
      " [ 0  6  0 56  2  1]\n",
      " [ 0  0  0  7  6  0]\n",
      " [ 0  0  0  4  1  3]]\n",
      "Training CM 4\n",
      "[[ 41   0   0  16   0   0   3]\n",
      " [  5   0   0  36   0   0   0]\n",
      " [  8   0   0  42   0   0   1]\n",
      " [  5   0   0 608   0   0   6]\n",
      " [  0   0   0  58   0   0   9]\n",
      " [  0   0   0  47   0   0  16]\n",
      " [  0   0   0  18   0   0 112]]\n",
      "Testing CM 4\n",
      "[[ 3  0  0  5  0  0  0]\n",
      " [ 0  0  0  6  0  0  1]\n",
      " [ 1  0  0  4  0  0  0]\n",
      " [ 2  0  0 58  0  0  2]\n",
      " [ 0  0  0  6  0  0  1]\n",
      " [ 0  0  0  6  0  0  1]\n",
      " [ 0  0  0  4  0  0  3]]\n",
      "Training CM 5\n",
      "[[  0   0  20   5   0   0   0]\n",
      " [  0   0  90   8   0   1   0]\n",
      " [  0   0  29  39   0   0   0]\n",
      " [  0   0  23 558   0  13   0]\n",
      " [  0   0   1  60   0  15   0]\n",
      " [  0   0   0  15   0  82  17]\n",
      " [  0   0   0   7   0   4  44]]\n",
      "Testing CM 5\n",
      "[[ 0  0  2  2  0  0  0]\n",
      " [ 0  0  4  4  0  1  0]\n",
      " [ 0  0  3  6  0  0  0]\n",
      " [ 0  0  4 54  0  4  1]\n",
      " [ 0  0  1  6  0  1  0]\n",
      " [ 0  0  1  2  0  3  1]\n",
      " [ 0  0  0  0  0  0  3]]\n",
      "Training CM 6\n",
      "[[ 44   0   0  11   0   0   0]\n",
      " [ 51   0   0  21   0   0   2]\n",
      " [  4   0   0  91   0   0   2]\n",
      " [  2   0   0 515   0   0   2]\n",
      " [  2   0   0  74   0   0   3]\n",
      " [  0   0   0  39   0   0  74]\n",
      " [  0   0   0  14   0   0  80]]\n",
      "Testing CM 6\n",
      "[[ 2  0  0  5  0  0  0]\n",
      " [ 1  0  0  6  0  0  1]\n",
      " [ 3  0  0 12  0  0  3]\n",
      " [ 2  0  0 45  0  0  1]\n",
      " [ 1  0  0  8  0  0  0]\n",
      " [ 1  0  0  5  0  0  2]\n",
      " [ 0  0  0  4  0  0  1]]\n",
      "Training CM 7\n",
      "[[  0  17   8   1   0   0   0]\n",
      " [  0   9  12   1   0   0   0]\n",
      " [  0   4 138  17   2   0   0]\n",
      " [  0   1  23 479  20   0   1]\n",
      " [  0   1   7  32 164   0   2]\n",
      " [  0   0   0   2  13   0  25]\n",
      " [  0   0   0   0   5   0  47]]\n",
      "Testing CM 7\n",
      "[[ 0  1  3  0  0  0  0]\n",
      " [ 0  0  1  1  0  0  0]\n",
      " [ 0  3  6  4  1  0  0]\n",
      " [ 0  0  3 37  9  0  0]\n",
      " [ 0  1  4  9  6  0  2]\n",
      " [ 0  0  0  1  2  0  1]\n",
      " [ 0  0  0  1  3  0  4]]\n",
      "Training CM 8\n",
      "[[  0   0  55   4   3   0   1]\n",
      " [  0   0  53   4   0   0   0]\n",
      " [  0   0  48   9   1   0   0]\n",
      " [  0   0  57 504  36   0   7]\n",
      " [  0   0   4  20  42   0   6]\n",
      " [  0   0   2   3  14   0  72]\n",
      " [  0   0   8   7   7   0  64]]\n",
      "Testing CM 8\n",
      "[[ 0  0  1  6  0  0  2]\n",
      " [ 0  0  6  1  0  0  1]\n",
      " [ 0  0  3  1  0  0  0]\n",
      " [ 0  0 11 38  4  0  5]\n",
      " [ 0  0  3  3  0  0  3]\n",
      " [ 0  0  0  1  1  0  9]\n",
      " [ 0  0  2  1  1  0  0]]\n",
      "Training CM 9\n",
      "[[  0  66   8   1   0   0   0]\n",
      " [  0  12   1   0   0   0   0]\n",
      " [  0  15 213  29   0   0   1]\n",
      " [  0   4  30 391   0  10   2]\n",
      " [  0   0   3  91   0  64   2]\n",
      " [  0   0   0   1   0   7  15]\n",
      " [  0   0   0   1   0  10  54]]\n",
      "Testing CM 9\n",
      "[[ 0  3  3  2  0  0  0]\n",
      " [ 0  3  1  0  0  0  0]\n",
      " [ 0  0 13  5  0  0  0]\n",
      " [ 0  2  6 32  0  1  2]\n",
      " [ 0  0  0 14  0  7  0]\n",
      " [ 0  0  1  1  0  0  1]\n",
      " [ 0  1  1  0  0  0  4]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "for param in list(class_scores_mod.keys()):\n",
    "    print('Training CM '+param)\n",
    "    y_pred = models[param].predict(X_train)\n",
    "    cm = confusion_matrix(y_train[param].argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    print(cm) \n",
    "    print('Testing CM '+param)\n",
    "    y_pred = models[param].predict(X_test)\n",
    "    cm = confusion_matrix(y_test[param].argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    print(cm) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on custom sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "WOH8JkBykXg4",
    "outputId": "e6568c16-f5fe-40c4-a0d9-e80256ffe098"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You Expand your energy, express your desires and anger\n",
      "1 2\n",
      "2 0\n",
      "3 0\n",
      "4 0\n",
      "5 0\n",
      "6 0\n",
      "7 0\n",
      "8 3\n",
      "9 0\n",
      "Expand your energy, express your desires and anger\n",
      "1 2\n",
      "2 0\n",
      "3 0\n",
      "4 0\n",
      "5 0\n",
      "6 0\n",
      "7 0\n",
      "8 3\n",
      "9 0\n"
     ]
    }
   ],
   "source": [
    "sent1= 'You Expand your energy, express your desires and anger'\n",
    "sent2= 'Expand your energy, express your desires and anger'\n",
    "\n",
    "x1= embedder.get(sent1).reshape((-1, 512))\n",
    "x2= embedder.get(sent2).reshape((-1, 512))\n",
    "\n",
    "print(sent1)\n",
    "for param in list(class_scores_mod.keys()):\n",
    "    #maxim= models[param].predict(x1).max(axis=1)\n",
    "    pred= np.argmax(models[param].predict(x1))-3\n",
    "    print(param, pred)\n",
    "\n",
    "print(sent2)\n",
    "for param in list(class_scores_mod.keys()):\n",
    "    #maxim= models[param].predict(x2).max(axis=1)\n",
    "    pred= np.argmax(models[param].predict(x2))-3\n",
    "    print(param, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "EN62BO-Wjhtk",
    "outputId": "fb34eae5-04c6-4f45-8a5c-45f6764897cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of      Unnamed: 0                                         Unnamed: 1  ...    8    9\n",
      "0           NaN                                                NaN  ...  NaN  NaN\n",
      "1           NaN  While addressing others' needs you are: Inflex...  ...  NaN  NaN\n",
      "2           NaN  While addressing others' needs you are: Flexib...  ...  NaN  NaN\n",
      "3           NaN                                                NaN  ...  NaN  NaN\n",
      "4           NaN                                                NaN  ...  NaN  NaN\n",
      "..          ...                                                ...  ...  ...  ...\n",
      "426         NaN  You Focus on others opinions or agendas and ca...  ...  NaN    #\n",
      "427         NaN                                                NaN  ...  NaN  NaN\n",
      "428         NaN                                                NaN  ...  NaN  NaN\n",
      "429         NaN    You Are clear about your views and are decisive  ...    #  NaN\n",
      "430         NaN  You Are unclear about your opinions, indecisiv...  ...  NaN    #\n",
      "\n",
      "[431 rows x 12 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(test_data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "H9fkL3jvoFWK",
    "outputId": "c68147b2-3b8a-43b7-d56d-336de766375d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inflexible and stick to your needs\n",
      "Flexible and alter your needs to make others happy\n",
      "Completely independent of others\n",
      "Interdependent and connected with others\n",
      "Overcome your personal terms and follow principles to do what is right\n",
      "Prefer personal terms, sometimes not going by principles\n",
      "Inner critic to do what is judged as right\n",
      "Drive to succeed and be recognized for your accomplishments\n",
      "find the method to do it perfectly and stick to it until it succeeds\n",
      "find the most efficient/ fastest possible method and in case it doesnot work , you change the method quickly to achieve the desired result\n",
      "tend to express your emotions openly and people are easily aware of it\n",
      "are not usually distracted by your feelings without people being aware of your emotional state\n",
      "tend to Suppress personal desires and become self constrained\n",
      "tend to Experience strong personal desires to the point of being self-absorbed (disconnected from outside world)\n",
      "Focus on responsibilities before emotions\n",
      "Sort out emotions and then complete duties effectively\n",
      "get irritated of others when They are inefficient ,sloppy\n",
      "get irritated of others when They are insensitive to oneself and you\n",
      "tend to suppress your desires or detach from feelings to To improve yourself and others\n",
      "tend to suppress your desires or detach from feelings to To protect yourself from being intruded and conserve your energy\n",
      "tend to judge people on Activities governed by their internal standards of right and wrong\n",
      "tend to judge people on Intellectual matters,knowledge and competence\n",
      "How do you deal with people who you think are wrong or whom you do not like you are convinced why you are correct and make efforts to make people understand that\n",
      "are not certain about your views and do not worry much if they do not agree with your view\n",
      "How do you prevent yourself from committing mistakes?  Comparing things to find out what is correct in order to avoid criticism\n",
      "How do you prevent yourself from committing mistakes?  Doubting and trying to find out the potential threats in order to gain sense of safety and security\n",
      "While taking decisions you are Mostly sure and have one right way of doing something\n",
      "While taking decisions you are generally unsure of the way to take and consider many ways of doing something\n",
      "can control negative feelings and keep negative feelings at bay\n",
      "find it difficult to ignore negative feelings\n",
      "are self restrained and limit your desires\n",
      "are are fun loving and strongly dislike limits on your desires\n",
      "How do you like to lead your life? Be self controlled,focussed , do not like to deviate from careful preparations/plans and particular about time and procedures,reserved\n",
      "How do you like to lead your life? Be adventurous,spontaneous, follow your inspiration and do not like to stick to plans,not particular about procedures,open minded,flexible\n",
      "know your mission clearly and have set high standards for achieving it , thus expect to be disappointed,practical\n",
      "How do you perceive your future? Your mission is a feeling/desire and you hope that you will achieve it , think positive,optimistic\n",
      "suppress anger and shows it when you think it is right\n",
      "express your anger on the spot and openly\n",
      "put logical reasons and arguments and try to convince them\n",
      "rely on your self confidence and sheer personality,engaging them with gusty conversations\n",
      "What is the first step in establishing justice according to you? To reward those who do good and punish the wrongdoers.\n",
      "What is the first step in establishing justice according to you? To establish equality in terms of power/authority\n",
      "Are not easily convinced to take a break,do not allow yourself to be unproductive\n",
      "Are easily convinced to take a break from work which will be useful\n",
      "Do not compromise with your principles to avoid conflicts\n",
      "Do not want to get into conflicts with your loved ones and hold on your strong opinions to yourself\n",
      "Hold on to your principles and ideals, always want to improve others and press for change\n",
      "Go along with others agendas and are more adaptable to their views and often lose sight of your own view\n",
      "Relationships and others’ needs and feelings\n",
      "Tasks and on completing goals\n",
      "What type of relationships do you prefer Close and intimate\n",
      "What type of relationships do you prefer Ordinary relationships, close relationships make you uncomfortable at times\n",
      "People are attracted towards you because of Your friendly nature and that you give attention to their feelings\n",
      "People are attracted towards you because of Your outstanding way of work and dealing with things\n",
      "Tend to move towards others and engage them\n",
      "Tend to withdraw from others hoping they would seek you\n",
      "Look for people to rescue them\n",
      "Look for someone to rescue you\n",
      "focus on and are aware of Others’ feelings and unaware of your needs\n",
      "focus on and are aware of Your feelings and fail to recognize your impact on others\n",
      "are Emotionally expressive and people oriented\n",
      "are Emotionally detached and loners\n",
      "If you are rejected by people you cope by Winning over them\n",
      "If you are rejected by people you cope by Detaching and isolating yourself\n",
      "Which one do you appreciate more in people Feelings\n",
      "Which one do you appreciate more in people Intellectual approaches and conceptual thinking\n",
      "In a relationship you want Love and sense of self worth\n",
      "In a relationship you want Certainty and security\n",
      "What do you want more To be loved and to be important to others\n",
      "What do you want more Approval and support of others\n",
      "In a new relationship or friendship you Welcome others and focus on their feelings, sometimes losing on your feelings\n",
      "In a new relationship or friendship you Welcome others but also doubt thems and protect yourself\n",
      "orient primarily towards Others needs, feelings and wants and alter yourself to meet them\n",
      "orient primarily towards Your feelings and likes and want to pursue them\n",
      "do not easily express anger and do not show wide range of feeling which are short lived\n",
      "With respect to expressing your feelings Your feelings are quite short lived and you display wide range of emotions like anger, frustration, delight or excitement\n",
      "try to rework and find it difficult to let go\n",
      "don’t like clinging on and feel letting go is better\n",
      "tend to focus on others and empower them based on Their feelings and wants; altering your beliefs\n",
      "tend to focus on others and empower them based on Your sense of justice that you feel is important\n",
      "find it difficult to show your feelings or anger and use indirect approaches\n",
      "directly communicate your aggression or disappointment\n",
      "tend to make others feel guilty of it may be by dramatizing the suffering\n",
      "push hard to get what you want may be by using direct threats\n",
      "tend to alter your views/opinions ( lose your real self) to be accepted and sometimes become over friendly\n",
      "are welcoming but do not alter your opinions to be accepted and also do not invade in people’s personal space\n",
      "want a bit of appreciation and love from others\n",
      "While focussing on others’ needs and feelings Attention or appreciation from others do not matter much may be because you want to be independent and safe from their influence\n",
      "tend to remind others how much they are indebted to you for your help\n",
      "tend to become silent or avoid further conflicts\n",
      "Are able to easily focus on your goals by letting go off your feelings or desires\n",
      "Are not able to focus on your goals and find it difficult to let go off your feelings and desires\n",
      "While feeling emotional at work you Focus your task and are able to ignore your feelings\n",
      "While feeling emotional at work you First need time to sort out your feelings and then proceed working\n",
      "Most of the time you find yourself to be Socially confident with positive self-image\n",
      "Most of the time you find yourself to be Socially uncomfortable with underconfident self image\n",
      "Are always up for work and work is more continuous for you with little or no breaks at all so that you are not perceived as sluggish\n",
      "Are energetic in intervals with distinct periods of withdrawal and recharge yourselves by thinking things over\n",
      "engage yourself in any task / subject To take it as a stepping stone to achieve your final goal and are quick to change your subject if it does not give you success or contribute in your final goal.\n",
      "engage yourself in any task / subject To acquire knowledge and learn and do not care about the final goal in doing so; also you will not stop doing a particular thing until you or the subject or both are exhausted by its content\n",
      "Care about having a good self image ( both looks and intellect) and work towards maintaining it\n",
      "Do not care about how you look or are seen as and follow your mind to acquire knowledge and do your work\n",
      "Are self mobilised to work towards your set goal to achieve appreciation and recognition and like them\n",
      "Are reluctant to start working as you are uncertain of your goal and fear pitfalls and are uncomfortable with appreciation and recognition and doubt them\n",
      "Want to be in the front ,known by all and in the limelight\n",
      "Do not want to be in the spotlight because with fame comes the fear of being blamed later for things that go wrong\n",
      "Choose the one which describes you more  Success oriented and optimistic\n",
      "Choose the one which describes you more  Security oriented and generally pessimistic\n",
      "Focus on your set goal and ignore your own pleasures in order to achieve success and maintain a good self image\n",
      "Focus on your own pleasures and desires and believe in keeping your options open in order to to achieve personal entitlement\n",
      "Focus on receiving constant stimulation, making worthwhile experiences and feel happy without any tendency to showing it off to anyone\n",
      "Focus on showing others that you are having a good time and might want others to feel jealous about it\n",
      "project the following image Cool, self controlled, perfectly together, without any problems\n",
      "project the following image Do not censor yourself and are funny, unrestricted, may be impulsive\n",
      "see failure As a personal humiliation and dread failures\n",
      "see failure As an opportunity to learn and do not highly fear failing\n",
      "For you Success and recognition is more important than power and authority\n",
      "For you Power and authority is more important than success and recognition\n",
      "are Highly competitive and do not argue with people to prove that you are right (conscious of your image)\n",
      "are Self assertive and argue with people to come to the right point\n",
      "Are fast paced, efficient , focussed and are impatient when obstruction comes their way in terms of others views, opinions etc\n",
      "Are slow paced,readily welcome others opinions and views and also may substitute their agendas over yours\n",
      "Are a self starter and take initiative to do things\n",
      "Find it difficult to self start and seek others to keep you motivated\n",
      "Want to seek attention of people who are important to you and work to get that\n",
      "feel it is awkward to ask for attention and distant yourself easily.\n",
      "Emotional and okay with sharing your feelings with others with less personal boundaries preferred\n",
      "Do not feel like sharing your emotions with many people and want to keep clear personal boundaries, might feel detached sometime\n",
      "Your work includes more of Imagination and fantasies, may be about you, your close relatives or based on your experiences\n",
      "Your work includes more of What you observe or see around, may be based on experiments. more about reality less autobiographical.\n",
      "try to undertstand the feelings or emotions\n",
      "try to comprehend the logics\n",
      "For you, It is okay to indulge into feelings and longings and think deeply about them to see what is missing that could be fulfilled\n",
      "For you, One should not be trapped in feelings and longings and think about what might go wrong in doing so\n",
      "Choose I am generally alone in life as I find it difficult to form bonds with others\n",
      "Choose I can make bonds with others but I fear losing them\n",
      "Which is more likely to make you depressed When you find that you have lost an opportunity to work on your dreams\n",
      "Which is more likely to make you depressed When you make your higher authority angry or disappointed with you\n",
      "Become gloomy and think deeply about emotions, even accepting negative emotions as a part of life\n",
      "Be happy and avoid pain or negative feelings\n",
      "It makes you appreciate the beauty of the place or appreciate the momento you bring with you\n",
      "It becomes a possession which helps you recall your accomplishment of going to that place, beauty of the object doesn’t matter much but experience does\n",
      "are excited and able to appreciate the beauty and possession for quite long\n",
      "are excited initially but soon lose interest and want to have something else very soon\n",
      "tend to Focus on your feelings that might lead to inactivity or losing direction\n",
      "tend to Overcome feeling to focus on work leading to neglection of your feelings at times\n",
      "find it difficult to let go of them and find it okay to take help of others to overcome\n",
      "think taking help would make you dependent on others and you yourself can get over it\n",
      "would describe yourself as  Soft, vulnerable, express emotions\n",
      "would describe yourself as  Tough, strong, repress emotions\n",
      "To protect yourself and give some time to yourself to deal with your emotions\n",
      "If you find them threatening or if they make you upset\n",
      "deal with all realities of life like tragedy, sadness, discomfort or happiness\n",
      "want the world to be happy and at peace, you find comfort in ordinary lives\n",
      "Choose one you can better relate to most of the time Pessimistic, moody and negative\n",
      "Choose one you can better relate to most of the time Optimistic, easy-going and positive\n",
      "detach from your feelings and delay your response to them\n",
      "magnify the danger and react immediately (often intensely) most of the times and find it difficult to detach from your feelings\n",
      "trust your own minds more than others\n",
      "try to find something to trust as you find it difficult to go by your mind alone\n",
      "Choose the one which describes you best Eccentric, ignore rules and seek knowledge\n",
      "Choose the one which describes you best Relate to tradition, consider rules and seek security\n",
      "tend to Withdraw yourself from others and you can spend hours alone, reading books, listening to music or working on your project\n",
      "tend to Stay active, might work with dedication but gets irritated sitting for too long\n",
      "tend to Feel that being optimistic always is unreal and that dark or painful side of life should also be thought upon deeply\n",
      "tend to Be optimistic and avoid dark or painful side of life\n",
      "Think before acting\n",
      "Generally act before thinking much\n",
      "Conserve your energy, reduce your desires\n",
      "Expand your energy, express your desires and anger\n",
      "Think before acting\n",
      "Generally act before thinking much\n",
      "Describe yourself  Sensitive, reserved and sometimes feel powerless\n",
      "Describe yourself  Insensitive, aggressive and often feel powerful\n",
      "tend to Detach from others in order to protect yourself or deal with your feelings\n",
      "tend to Find it difficult to detach from others rather blend with them to keep life comfortable and peaceful\n",
      "are Intense, defensive, strong-minded, and highly resistant to the influence of others\n",
      "are Gentle, patient, easygoing, accommodating and receptive\n",
      "Fear being overwhelmed by the world and try to defend yourself intellectually\n",
      "Feel at ease with the world and you desire to merge with others\n",
      "For you Pleasure and personal wants are secondary and there is no harm in staying within predefined limits..\n",
      "For you Pleasure and fulfilling personal wants are primary concerns and dislike limitations\n",
      "Look for pitfalls,dangers in situations before doing anything and plan for handling those problems\n",
      "Look for positive things in situations, believe in going with the flow and plan for multiple possibilities\n",
      "Which describes you better Pessimistic but committed and responsible\n",
      "Which describes you better Optimistic but irresponsible sometimes, forgetful and want freedom\n",
      "Are hesitant to act, magnify hazards and sometime give way under pressure or doubt\n",
      "Are confident to act,deny dangers and always hold their ground\n",
      "Describe yourself Anxious, unsure and self-doubting\n",
      "Describe yourself confident (sometimes over-confident), willful and self-assured\n",
      "Are often in doubt and seek assurance of others\n",
      "Are mostly always certain and not in doubt\n",
      "Are focussed on what could go wrong and maintain personal distance\n",
      "Are more people oriented and focus on their requests and claims\n",
      "Do not trust others easily and get along with others after careful questioning and tests\n",
      "Very welcoming, accepting and get along with others before testing or questioning\n",
      "Are fast paced in thinking and are quick to respond or react\n",
      "take time to process your thoughts and take time to to respond or react\n",
      "Tend to escape pain or conflicts, explain away difficulties and plan about future a lot\n",
      "Accept pain, confront conflicts directly and mostly stay in the present\n",
      "Choose the one that best describes you Playful and optimistic\n",
      "Choose the one that best describes you Hard working and realistic\n",
      "want  Variety, freedom and fun\n",
      "want  Intensity, control and power\n",
      "Are fast paced, short tempered and impulsive in action\n",
      "Are steady paced, even tempered and cautious in action\n",
      "Are self oriented and tend to express your own desires and opinions\n",
      "Are people oriented and consider others desires/ opinions and may forget your own\n",
      "are Hyperactive, involved and assertive\n",
      "are Passive, less responsive and compliant\n",
      "Accept conflicts and express anger directly\n",
      "Avoid conflicts and hide anger\n",
      "Focus on your own opinions or agendas and defend them strongly\n",
      "Focus on others opinions or agendas and can lose your own\n",
      "Are clear about your views and are decisive\n",
      "Are unclear about your opinions, indecisive and often go along with what others say\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data.\n",
    "test_sent= []\n",
    "test_target= []\n",
    "test_embds= []\n",
    "test_scores= {str(x): [] for x in range(1, 10)}\n",
    "\n",
    "for i in range(test_data.shape[0]):\n",
    "    if type(test_data.loc[i, 'Unnamed: 1'])==str:\n",
    "        sent= test_data.loc[i, 'Unnamed: 1']\n",
    "        test_sent.append(sent)\n",
    "        sent= sent.split(':')[-1].split('You ')[-1].strip()\n",
    "        print(sent)\n",
    "        embd= embedder.get(sent)\n",
    "        test_embds.append(embd)\n",
    "        test_target.append(test_data.loc[i].index[test_data.loc[i]=='#'][0])\n",
    "\n",
    "        for param in list(test_scores.keys()):\n",
    "            #maxim= models[param].predict(x2).max(axis=1)\n",
    "            pred= np.argmax(models[param].predict(embd.reshape((-1, 512))))-3\n",
    "            test_scores[param].append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z3KX4JkxZETh"
   },
   "outputs": [],
   "source": [
    "results= pd.DataFrame(test_sent)\n",
    "results= results.join(pd.DataFrame(test_scores))\n",
    "results.to_csv('enneagram_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MKRda1Wn5jao",
    "outputId": "18a0cfc9-95c9-4545-d8f0-3d9b895e4785"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['3' '8' '9']\n",
      " ['2' '9' '0']\n",
      " ['9' '6' '8']\n",
      " ['2' '9' '0']\n",
      " ['1' '7' '0']\n",
      " ['7' '0' '0']\n",
      " ['0' '0' '0']\n",
      " ['3' '8' '9']\n",
      " ['1' '5' '8']\n",
      " ['1' '3' '5']\n",
      " ['4' '2' '0']\n",
      " ['4' '2' '0']\n",
      " ['2' '6' '0']\n",
      " ['4' '0' '0']\n",
      " ['0' '0' '0']\n",
      " ['8' '1' '5']\n",
      " ['1' '5' '0']\n",
      " ['2' '0' '0']\n",
      " ['2' '4' '0']\n",
      " ['1' '2' '0']\n",
      " ['1' '0' '0']\n",
      " ['0' '0' '0']\n",
      " ['4' '1' '5']\n",
      " ['1' '0' '0']\n",
      " ['4' '0' '0']\n",
      " ['1' '5' '6']\n",
      " ['1' '5' '6']\n",
      " ['5' '6' '7']\n",
      " ['2' '8' '0']\n",
      " ['4' '2' '8']\n",
      " ['2' '0' '0']\n",
      " ['7' '2' '0']\n",
      " ['2' '5' '0']\n",
      " ['4' '5' '7']\n",
      " ['8' '0' '0']\n",
      " ['1' '8' '3']\n",
      " ['9' '1' '2']\n",
      " ['9' '8' '0']\n",
      " ['1' '5' '9']\n",
      " ['8' '0' '0']\n",
      " ['1' '7' '0']\n",
      " ['1' '0' '0']\n",
      " ['5' '7' '0']\n",
      " ['8' '0' '0']\n",
      " ['1' '9' '0']\n",
      " ['2' '0' '0']\n",
      " ['1' '7' '9']\n",
      " ['1' '7' '0']\n",
      " ['2' '0' '0']\n",
      " ['3' '8' '0']\n",
      " ['2' '0' '0']\n",
      " ['6' '2' '0']\n",
      " ['1' '2' '0']\n",
      " ['2' '1' '0']\n",
      " ['1' '5' '7']\n",
      " ['9' '6' '8']\n",
      " ['9' '2' '0']\n",
      " ['9' '2' '8']\n",
      " ['2' '0' '0']\n",
      " ['2' '0' '0']\n",
      " ['4' '0' '0']\n",
      " ['2' '0' '0']\n",
      " ['6' '2' '7']\n",
      " ['6' '2' '7']\n",
      " ['2' '4' '9']\n",
      " ['4' '5' '0']\n",
      " ['2' '0' '0']\n",
      " ['2' '1' '0']\n",
      " ['2' '4' '0']\n",
      " ['2' '6' '8']\n",
      " ['4' '2' '7']\n",
      " ['9' '2' '0']\n",
      " ['2' '7' '0']\n",
      " ['2' '7' '0']\n",
      " ['4' '0' '0']\n",
      " ['4' '0' '0']\n",
      " ['4' '3' '5']\n",
      " ['4' '2' '7']\n",
      " ['1' '7' '0']\n",
      " ['1' '7' '0']\n",
      " ['0' '0' '0']\n",
      " ['9' '0' '0']\n",
      " ['2' '0' '0']\n",
      " ['3' '6' '8']\n",
      " ['2' '6' '0']\n",
      " ['2' '0' '0']\n",
      " ['2' '9' '0']\n",
      " ['2' '0' '0']\n",
      " ['1' '2' '9']\n",
      " ['0' '0' '0']\n",
      " ['4' '2' '0']\n",
      " ['4' '2' '7']\n",
      " ['2' '0' '0']\n",
      " ['4' '0' '0']\n",
      " ['0' '0' '0']\n",
      " ['2' '0' '0']\n",
      " ['3' '1' '5']\n",
      " ['1' '5' '0']\n",
      " ['8' '3' '0']\n",
      " ['1' '6' '0']\n",
      " ['2' '1' '0']\n",
      " ['1' '5' '0']\n",
      " ['2' '8' '7']\n",
      " ['1' '0' '0']\n",
      " ['4' '0' '0']\n",
      " ['4' '1' '6']\n",
      " ['2' '8' '7']\n",
      " ['0' '0' '0']\n",
      " ['1' '7' '8']\n",
      " ['7' '0' '0']\n",
      " ['4' '0' '0']\n",
      " ['2' '0' '0']\n",
      " ['4' '0' '0']\n",
      " ['0' '0' '0']\n",
      " ['8' '6' '0']\n",
      " ['6' '7' '0']\n",
      " ['1' '3' '9']\n",
      " ['1' '2' '9']\n",
      " ['0' '0' '0']\n",
      " ['1' '0' '0']\n",
      " ['1' '5' '0']\n",
      " ['4' '9' '0']\n",
      " ['1' '5' '8']\n",
      " ['6' '7' '8']\n",
      " ['2' '0' '0']\n",
      " ['4' '2' '0']\n",
      " ['2' '9' '0']\n",
      " ['4' '2' '0']\n",
      " ['4' '0' '0']\n",
      " ['5' '0' '0']\n",
      " ['2' '0' '0']\n",
      " ['5' '7' '0']\n",
      " ['4' '2' '0']\n",
      " ['2' '0' '0']\n",
      " ['2' '0' '0']\n",
      " ['2' '6' '0']\n",
      " ['4' '3' '0']\n",
      " ['1' '0' '0']\n",
      " ['4' '8' '0']\n",
      " ['2' '0' '0']\n",
      " ['4' '0' '0']\n",
      " ['4' '7' '0']\n",
      " ['4' '0' '0']\n",
      " ['4' '7' '0']\n",
      " ['4' '8' '0']\n",
      " ['4' '0' '0']\n",
      " ['4' '2' '0']\n",
      " ['2' '0' '0']\n",
      " ['0' '0' '0']\n",
      " ['2' '0' '0']\n",
      " ['2' '0' '0']\n",
      " ['9' '1' '2']\n",
      " ['4' '0' '0']\n",
      " ['2' '0' '0']\n",
      " ['4' '0' '0']\n",
      " ['0' '0' '0']\n",
      " ['1' '0' '0']\n",
      " ['4' '0' '0']\n",
      " ['2' '9' '0']\n",
      " ['0' '0' '0']\n",
      " ['1' '5' '0']\n",
      " ['1' '7' '0']\n",
      " ['4' '7' '0']\n",
      " ['4' '6' '9']\n",
      " ['4' '1' '6']\n",
      " ['4' '0' '0']\n",
      " ['8' '6' '0']\n",
      " ['1' '4' '6']\n",
      " ['8' '0' '0']\n",
      " ['8' '0' '0']\n",
      " ['8' '6' '0']\n",
      " ['1' '4' '6']\n",
      " ['2' '1' '0']\n",
      " ['2' '0' '0']\n",
      " ['2' '0' '0']\n",
      " ['4' '5' '7']\n",
      " ['0' '0' '0']\n",
      " ['2' '9' '0']\n",
      " ['9' '0' '0']\n",
      " ['2' '9' '8']\n",
      " ['4' '6' '0']\n",
      " ['0' '0' '0']\n",
      " ['1' '6' '9']\n",
      " ['7' '0' '0']\n",
      " ['6' '0' '0']\n",
      " ['4' '0' '0']\n",
      " ['1' '4' '0']\n",
      " ['4' '0' '0']\n",
      " ['6' '0' '0']\n",
      " ['1' '0' '0']\n",
      " ['1' '0' '0']\n",
      " ['9' '6' '0']\n",
      " ['1' '4' '0']\n",
      " ['2' '9' '0']\n",
      " ['2' '9' '1']\n",
      " ['9' '0' '0']\n",
      " ['9' '1' '0']\n",
      " ['4' '1' '8']\n",
      " ['6' '5' '7']\n",
      " ['9' '8' '0']\n",
      " ['2' '0' '0']\n",
      " ['8' '3' '0']\n",
      " ['0' '0' '0']\n",
      " ['8' '9' '0']\n",
      " ['4' '0' '0']\n",
      " ['4' '8' '5']\n",
      " ['2' '0' '0']\n",
      " ['4' '2' '0']\n",
      " ['5' '0' '0']\n",
      " ['8' '0' '0']\n",
      " ['2' '8' '0']\n",
      " ['8' '9' '0']\n",
      " ['1' '7' '0']\n",
      " ['7' '0' '0']\n",
      " ['1' '6' '7']\n",
      " ['0' '0' '0']]\n"
     ]
    }
   ],
   "source": [
    "# Get top3 scores.\n",
    "top3= []\n",
    "\n",
    "for i in range(results.shape[0]):\n",
    "    inds= results.loc[i].index[results.loc[i]==3].tolist()\n",
    "    if len(inds)==0:\n",
    "        inds= results.loc[i].index[results.loc[i]==-3].tolist()\n",
    "        inds= inds+results.loc[i].index[results.loc[i]==-2].tolist()\n",
    "        inds= inds+results.loc[i].index[results.loc[i]==-1].tolist()+['0', '0', '0']\n",
    "    else:\n",
    "        inds= inds+results.loc[i].index[results.loc[i]==-2].tolist()\n",
    "        inds= inds+results.loc[i].index[results.loc[i]==-1].tolist()+['0', '0', '0']\n",
    "    inds= inds[:3]\n",
    "    top3.append(inds)\n",
    "\n",
    "top3= np.asarray(top3)\n",
    "print(top3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mO0RHkpYSLOi",
    "outputId": "de51855f-ddad-4a26-fd18-f74816e8c259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39814814814814814\n"
     ]
    }
   ],
   "source": [
    "top3_score= 0\n",
    "top2_score= 0\n",
    "top1_score= 0\n",
    "\n",
    "for i in range(top3.shape[0]):\n",
    "    if test_target[i] in top3[i]:\n",
    "        top3_score+= 1\n",
    "    if test_target[i] in top3[i, :2]:\n",
    "        top2_score+= 1\n",
    "    if test_target[i] == top3[i, 0]:\n",
    "        top1_score+= 1\n",
    "\n",
    "top3_score/= 216\n",
    "top2_score/= 216\n",
    "top1_score/= 216\n",
    "\n",
    "print(top3_score)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "enneagram.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
